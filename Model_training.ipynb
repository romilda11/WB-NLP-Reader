{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import thulac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_id</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>content</th>\n",
       "      <th>polar</th>\n",
       "      <th>wb_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4650</td>\n",
       "      <td>1</td>\n",
       "      <td>#菲军舰恶意撞击#又他妈一个猴子</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4657</td>\n",
       "      <td>1</td>\n",
       "      <td>#菲军舰恶意撞击#搞死它死菲佣</td>\n",
       "      <td>NEG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658</td>\n",
       "      <td>1</td>\n",
       "      <td>#菲军舰恶意撞击#哇靠！</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658</td>\n",
       "      <td>2</td>\n",
       "      <td>究竟还要忍多久！</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658</td>\n",
       "      <td>3</td>\n",
       "      <td>我们的军队是摆设吗？</td>\n",
       "      <td>NEG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weibo_id sen_id           content polar wb_length\n",
       "0     4650      1  #菲军舰恶意撞击#又他妈一个猴子   NEG         1\n",
       "1     4657      1   #菲军舰恶意撞击#搞死它死菲佣   NEG         1\n",
       "2     4658      1      #菲军舰恶意撞击#哇靠！  None         3\n",
       "3     4658      2          究竟还要忍多久！  None         3\n",
       "4     4658      3        我们的军队是摆设吗？   NEG         3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load all the xml files in the directory and decode them\n",
    "paths = []\n",
    "rootset = []\n",
    "for path, subdirs, files in os.walk(r'C:\\Users\\romilda\\Desktop\\weibo scrapers\\training'):\n",
    "    for filename in files:\n",
    "        f = os.path.join(path, filename)\n",
    "        data = ET.parse(f).getroot()\n",
    "        rootset.append(data)\n",
    "#save the text data into dataframes\n",
    "weibo_id = []\n",
    "sen_id = []\n",
    "content = []\n",
    "polar = []\n",
    "wb_len = []\n",
    "for sets in rootset:\n",
    "    for wb in sets:\n",
    "        for stnc in wb:\n",
    "            #print(str(stnc))\n",
    "            weibo_id.append(re.findall(\"\"\"{'id': '(.*)'\"\"\",str(wb.attrib))[0])\n",
    "            sen_id.append(str(stnc.get('id')))\n",
    "            content.append(str(stnc.text))\n",
    "            polar.append(str(stnc.get('polarity')))\n",
    "            wb_len.append(str(len(wb)))\n",
    "dt = pd.DataFrame({'weibo_id':weibo_id,\n",
    "                  'sen_id':sen_id,\n",
    "                  'content':content,\n",
    "                  'polar':polar,\n",
    "                  'wb_length':wb_len})\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that combine sentences for each paragraph\n",
    "def combine_sen(input_dt,i,sen_len,weibo_id,output_dt):\n",
    "    j = output_dt['weibo_id'].count()+1 #to get the index of next record according to the size of existing output dataset\n",
    "    temp = [weibo_id] #temp is the piece of record written to output dataset; write weibo_id into the record\n",
    "    \n",
    "    content = ''\n",
    "    polar = 0\n",
    "    for x in range(sen_len):\n",
    "        content = content+' '+str(input_dt.iat[i,2]) #combine the sentences\n",
    "        \n",
    "        #get the overall polarity of the whole sentence\n",
    "        if str(input_dt.iat[i,3])=='NEG':\n",
    "            polar = polar-1\n",
    "        elif str(input_dt.iat[i,3])=='POS':\n",
    "            polar = polar+1\n",
    "        \n",
    "        i=i+1 #update the location of next piece of data\n",
    "    \n",
    "    temp.append(content)  #write the content into the record\n",
    "    \n",
    "    #write polarity of the paragraph into the record\n",
    "    if polar>0:\n",
    "        pl = 1 #here 1 means positive\n",
    "    elif polar==0:\n",
    "        pl = 0 #here 0 means neutral\n",
    "    else:\n",
    "        pl = -1 #here -1 means negative\n",
    "    temp.append(pl)\n",
    "    output_dt.loc[j] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_id</th>\n",
       "      <th>content</th>\n",
       "      <th>polar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4650</td>\n",
       "      <td>#菲军舰恶意撞击#又他妈一个猴子</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4657</td>\n",
       "      <td>#菲军舰恶意撞击#搞死它死菲佣</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658</td>\n",
       "      <td>#菲军舰恶意撞击#哇靠！ 究竟还要忍多久！ 我们的军队是摆设吗？</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weibo_id                            content polar\n",
       "1     4650                   #菲军舰恶意撞击#又他妈一个猴子    -1\n",
       "2     4657                    #菲军舰恶意撞击#搞死它死菲佣    -1\n",
       "3     4658   #菲军舰恶意撞击#哇靠！ 究竟还要忍多久！ 我们的军队是摆设吗？    -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine the sentences\n",
    "dts = pd.DataFrame(columns=['weibo_id','content','polar'])\n",
    "total_rows = dt['weibo_id'].count()\n",
    "i = 0\n",
    "while i < total_rows: #\n",
    "    sen_len = int(dt.iat[i,4])\n",
    "    wb_id = dt.at[i,'weibo_id']\n",
    "    combine_sen(dt,i,sen_len,wb_id,dts)\n",
    "    i = i + sen_len\n",
    "dts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_id</th>\n",
       "      <th>content</th>\n",
       "      <th>polar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4650</td>\n",
       "      <td>菲 军舰 恶意 撞击 又 他 妈 一个 猴子</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4657</td>\n",
       "      <td>菲 军舰 恶意 撞击 搞 死 它 死 菲 佣</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658</td>\n",
       "      <td>菲 军舰 恶意 撞击 哇 靠   究竟 还要 忍 多久   我们 的 军队 是 摆设 吗</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weibo_id                                         content polar\n",
       "1     4650                          菲 军舰 恶意 撞击 又 他 妈 一个 猴子    -1\n",
       "2     4657                          菲 军舰 恶意 撞击 搞 死 它 死 菲 佣    -1\n",
       "3     4658    菲 军舰 恶意 撞击 哇 靠   究竟 还要 忍 多久   我们 的 军队 是 摆设 吗    -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load stopwords which are mostly special characters, clean it for later use\n",
    "st =  pd.read_excel(r'C:\\Users\\romilda\\Desktop\\weibo scrapers\\\\stopword.xlsx',header=None)\n",
    "stopword = []\n",
    "colname = st.columns[0]\n",
    "for index,row in st.iterrows():\n",
    "    stopword.append(row[colname].replace('\\xa0',''))\n",
    "    \n",
    "#word segmentation\n",
    "ud1 = r'C:\\Users\\romilda\\Desktop\\weibo scrapers\\userdict.txt'#import userdict\n",
    "thu1 = thulac.thulac(user_dict=ud1,seg_only=True)\n",
    "for index,row in dts.iterrows():\n",
    "    temp = thu1.cut(row['content'])\n",
    "    words = '' \n",
    "    for item in temp:\n",
    "        if item[0] not in stopword:\n",
    "            words = words+' '+item[0]\n",
    "        dts.loc[index,'content'] = words[1:]\n",
    "dts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = dts[['content','polar']] #save only columns of content and polarity\n",
    "ot.to_csv('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\seg_train.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "343\n",
      "350\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 94, 192)           1248000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 94, 192)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               304976    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 1,553,567\n",
      "Trainable params: 1,553,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 4s - loss: 1.0966 - acc: 0.3631\n",
      "Epoch 2/10\n",
      " - 2s - loss: 1.0799 - acc: 0.4750\n",
      "Epoch 3/10\n",
      " - 2s - loss: 1.0196 - acc: 0.5416\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.9525 - acc: 0.6324\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.8290 - acc: 0.6989\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.7958 - acc: 0.7020\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.6846 - acc: 0.7988\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.6014 - acc: 0.8351\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.4960 - acc: 0.8578\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.4249 - acc: 0.8593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25642842e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to restart the kernel everytime building the model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "#prepare the train set and test set\n",
    "inpt = pd.read_csv(r'C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\seg_train.csv',header='infer')\n",
    "pos = inpt[inpt['polar'] == 1]\n",
    "neur = inpt[inpt['polar'] == 0]\n",
    "neg= inpt[inpt['polar'] == -1].sample(n=350)\n",
    "rd = pd.concat([pos,neur,neg],axis=0)\n",
    "rd['content']= rd['content'].apply(lambda x: x.lower()) #convert capital letters into lower case\n",
    "print(int(rd[rd['polar'] == 1].size/2)) #print the number of rows with positive label\n",
    "print(int(rd[rd['polar'] == 0].size/2)) #print the number of rows with neural label\n",
    "print(int(rd[rd['polar'] == -1].size/2)) #print the number of rows with negative label\n",
    "tk = keras.preprocessing.text.Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=True)\n",
    "tk.fit_on_texts(rd['content'].values)\n",
    "X = tk.texts_to_sequences(rd['content'].values)\n",
    "X = pad_sequences(X)\n",
    "#save the dictionary of tokenizer which will be loaded for prediction data\n",
    "with open('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "Y = pd.get_dummies(rd['polar']).values #convert the labels into dummy variables that machine can understand\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42) #67% data is used as train data and 33% is used as test data\n",
    "\n",
    "#build the model\n",
    "max_features = 6500\n",
    "embed_dim = 192 #128\n",
    "lstm_out = 196\n",
    "batch_size_ = 84 # the maximum length is 112\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#train the model with training set\n",
    "model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size_, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411042944785276"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to calculate the accuracy of prediction on train data \n",
    "y=model.predict(X_test)\n",
    "label=y.argmax(axis=1)\n",
    "# 0: -1 1: 0  2: 1\n",
    "sum(label==0)\n",
    "sum(label==1)\n",
    "sum(label==2)\n",
    "Ytest_label=np.array(list(map(lambda x: list(x).index(1),Y_test)))\n",
    "(Ytest_label==label).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the architecture of the model, the weight, the training configuration (loss, optimizer) and the state of the optimizer\n",
    "model.save('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section of code is not needed for the project, but may be used if need to load the model for further use\n",
    "from keras.models import load_model\n",
    "n_model = load_model('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = keras.preprocessing.text.Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=True)\n",
    "# load the dictionary of indexing from previous training process\n",
    "with open('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\tokenizer.pickle', 'rb') as handle:\n",
    "    tk = pickle.load(handle)\n",
    "    \n",
    "# function that converts segmented words into index with the same dictionary as data training\n",
    "def convert_to_sq(lst):\n",
    "    x = tk.texts_to_sequences(lst)\n",
    "    x = pad_sequences(x, maxlen=94)\n",
    "    return x\n",
    "\n",
    "# prediction with comments as input\n",
    "km = pd.read_excel(r'C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\seg_comments_with_index.xlsx')\n",
    "yypred = np.zeros((len(km),3))\n",
    "for i in range(len(km)):\n",
    "    pdc = convert_to_sq([str(km['content'][i])])\n",
    "    yypred[i]=model.predict(pdc,batch_size=1)\n",
    "label=yypred.argmax(axis=1)-1\n",
    "pl = pd.DataFrame(label,columns=['pred_polarity'])\n",
    "fc = pd.concat([km,pl],axis=1)\n",
    "fc.to_excel('C:\\\\Users\\\\romilda\\\\Desktop\\\\weibo scrapers\\\\pred_comments.xlsx',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
